{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicts to identify which countries go with which tickers (all are 10-year Govt yields)\n",
    "codes = {}\n",
    "codes['US'] = 'USGG10YR'\n",
    "codes['Germany'] = 'GDBR10'\n",
    "codes['UK'] = 'GUKG10'\n",
    "codes['France'] = 'GFRN10'\n",
    "codes['Australia'] = 'GACGB10'\n",
    "codes['Canada'] = 'GCAN10YR'\n",
    "codes['New Zealand'] = 'GNZGB10'\n",
    "codes['Japan'] = 'JGBS10'\n",
    "codes['Switzerland'] = 'GSWISS10'\n",
    "codes['Norway'] = 'GNOR10YR'\n",
    "codes['Italy'] = 'GBTPGR10'\n",
    "\n",
    "codes_back = {}\n",
    "for key, value in codes.items():\n",
    "    codes_back[value] = key\n",
    "\n",
    "sheet_names = pd.ExcelFile('G10_RV.xlsx').sheet_names\n",
    "\n",
    "# Combining data into single df\n",
    "for i, x in enumerate(sheet_names[:11]):\n",
    "    if i == 0:\n",
    "        df = pd.read_excel('G10_RV.xlsx', sheet_name=x)[['Date', 'Last Price']]\n",
    "        df.columns = ['Date', x]\n",
    "    else:\n",
    "        new_df = pd.read_excel('G10_RV.xlsx', sheet_name=x)[['Date', 'Last Price']]\n",
    "        new_df.columns = ['Date', x]\n",
    "        df = df.merge(new_df, on='Date', how='outer')\n",
    "\n",
    "# Filling in missing days with previous observations, defining which columns are rates we want\n",
    "df = df.set_index('Date')\n",
    "df = df.resample('D').asfreq()\n",
    "df = df.ffill()\n",
    "df = df[::-1].dropna()\n",
    "rates_tickers = df.columns[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictor</th>\n",
       "      <th>y: US</th>\n",
       "      <th>y: Germany</th>\n",
       "      <th>y: UK</th>\n",
       "      <th>y: France</th>\n",
       "      <th>y: Australia</th>\n",
       "      <th>y: Canada</th>\n",
       "      <th>y: New Zealand</th>\n",
       "      <th>y: Japan</th>\n",
       "      <th>y: Switzerland</th>\n",
       "      <th>y: Norway</th>\n",
       "      <th>y: Italy</th>\n",
       "      <th>ABS Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.16</td>\n",
       "      <td>None</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Canada</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>None</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>None</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japan</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Norway</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Italy</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>None</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Training r2</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.55</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Testing r2</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.73</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predictor y: US y: Germany y: UK y: France y: Australia y: Canada  \\\n",
       "0            US  None        0.1   0.4      0.02        -0.09       0.5   \n",
       "1       Germany   0.2       None  0.46       0.6          0.1      0.01   \n",
       "2            UK  0.29       0.17  None      0.08        -0.06     -0.13   \n",
       "3        France  0.03       0.43  0.16      None         0.16     -0.15   \n",
       "4     Australia -0.08       0.04 -0.07       0.1         None      0.29   \n",
       "5        Canada  0.54       0.01 -0.19     -0.12         0.36      None   \n",
       "6   New Zealand  0.07      -0.11  0.25     -0.01         0.47      0.03   \n",
       "7         Japan  0.32       0.06 -0.06      0.16        -0.06     -0.05   \n",
       "8   Switzerland  -0.1       0.39 -0.16      0.08         0.26      0.04   \n",
       "9        Norway  0.11       0.05  0.16      0.03         0.03      0.23   \n",
       "10        Italy -0.02      -0.04 -0.05      0.19        -0.04      0.04   \n",
       "11                                                                        \n",
       "12  Training r2  0.89       0.92  0.85       0.9         0.88      0.87   \n",
       "13   Testing r2  0.72       0.92  0.54      0.97         0.61      0.84   \n",
       "\n",
       "   y: New Zealand y: Japan y: Switzerland y: Norway y: Italy ABS Mean  \n",
       "0            0.11     0.11          -0.07      0.13    -0.14    0.167  \n",
       "1           -0.32     0.04           0.49      0.13    -0.64    0.299  \n",
       "2            0.28    -0.01          -0.08      0.14    -0.32    0.156  \n",
       "3           -0.01     0.08           0.07      0.05     2.19    0.333  \n",
       "4            0.61    -0.02           0.14      0.04    -0.28    0.167  \n",
       "5            0.05    -0.02           0.03       0.3     0.34    0.196  \n",
       "6            None     0.04           0.03      0.11     0.09    0.121  \n",
       "7            0.17     None           0.13     -0.25    -0.17    0.143  \n",
       "8            0.08     0.07           None      0.12    -0.17    0.147  \n",
       "9            0.14    -0.07           0.06      None    -0.21    0.109  \n",
       "10           0.02    -0.01          -0.01     -0.03     None    0.045  \n",
       "11                                                                     \n",
       "12           0.85     0.47           0.83      0.82     0.55           \n",
       "13           0.78     0.14           0.14       0.8     0.73           "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple(t):\n",
    "    data = df[rates_tickers].copy()\n",
    "    # Calculating our changes\n",
    "    for ticker in data:\n",
    "        data[f'{ticker}_c'] = data[ticker].diff(-t)\n",
    "\n",
    "    #Train, test split\n",
    "    data = data.dropna()\n",
    "    changes = data[[x for x in data if x.endswith('_c')]]\n",
    "    changes_training = changes[changes.index < '2023-1-1']\n",
    "    changes_testing = changes[changes.index >= '2023-1-1']\n",
    "    causalities = pd.DataFrame()\n",
    "    causalities['Predictor'] = [codes_back[x] for x in rates_tickers] + ['', 'Training r2', 'Testing r2']\n",
    "\n",
    "    # Building model and storing results for each rate as the target, making df to see results\n",
    "    for i, target in enumerate(changes.columns):\n",
    "        training_X = changes_training[[x for x in changes_training if x != target]]\n",
    "        training_y = changes_training[target]\n",
    "        testing_X = changes_testing[[x for x in changes_testing if x != target]]\n",
    "        testing_y = changes_testing[target]\n",
    "        model = LinearRegression()\n",
    "        model.fit(training_X, training_y)\n",
    "        training_prediction = model.predict(training_X)\n",
    "        testing_prediction = model.predict(testing_X)\n",
    "        training_r2 = r2_score(training_y, training_prediction)\n",
    "        testing_r2 = r2_score(testing_y, testing_prediction)\n",
    "        coefficients = [round(x,2) for x in model.coef_]\n",
    "        coefficients.insert(i, None)\n",
    "        coefficients.insert(len(coefficients), '')\n",
    "        coefficients.insert(len(coefficients), round(training_r2, 2))\n",
    "        coefficients.insert(len(coefficients), round(testing_r2, 2))\n",
    "        causalities[f'y: {codes_back[target[:-2]]}'] = coefficients\n",
    "    causalities['ABS Mean'] = [causalities.iloc[i, 1:].dropna().abs().mean() for i in range(11)] + ['', '', '']\n",
    "    \n",
    "    return causalities\n",
    "\n",
    "simple(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_values = [1,5,10,25,50,100]\n",
    "\n",
    "def multi(target, t):\n",
    "    target_t = f'{target}_{t}'\n",
    "    data = df[rates_tickers].copy()\n",
    "    # Adding changes for each t value we specified for every ticker\n",
    "    for ticker in rates_tickers:\n",
    "        for x in t_values:\n",
    "            data[f'{ticker}_{x}'] = data[ticker].diff(-x)\n",
    "\n",
    "    #Training model and storing predictions and performance. Too many predictors to make clean df\n",
    "    data = data.dropna()\n",
    "    data_training = data[data.index < '2023-1-1'].copy()\n",
    "    data_testing = data[data.index >= '2023-1-1'].copy()\n",
    "    training_X = data_training[[x for x in data_training if '_' in x and x != target_t]]\n",
    "    training_y = data_training[target_t]\n",
    "    testing_X = data_testing[[x for x in data_testing if '_' in x and x != target_t]]\n",
    "    testing_y = data_testing[target_t]\n",
    "    model = LinearRegression()\n",
    "    model.fit(training_X, training_y)\n",
    "    testing_prediction = model.predict(testing_X)\n",
    "    r2 = round(r2_score(testing_y, testing_prediction),2)\n",
    "    data_testing['c_prediction'] = testing_prediction\n",
    "    prediction = data_testing[[target, target_t, 'c_prediction']].copy()\n",
    "    prediction['prediction'] = prediction[target].shift(-t) + prediction['c_prediction']\n",
    "    prediction = prediction[[target, 'prediction']].dropna()\n",
    "    return prediction, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_values = [1,5,10,25,50,100]\n",
    "\n",
    "def nn_multi(target, t):\n",
    "    target_t = f'{target}_{t}'\n",
    "    data = df[rates_tickers].copy()\n",
    "    for ticker in rates_tickers:\n",
    "        for x in t_values:\n",
    "            data[f'{ticker}_{x}'] = data[ticker].diff(-x)\n",
    "    data = data.dropna()\n",
    "    data_training = data[data.index < '2023-1-1'].copy()\n",
    "    data_testing = data[data.index >= '2023-1-1'].copy()\n",
    "    training_X = data_training[[x for x in data_training if '_' in x and x != target_t]]\n",
    "    training_y = data_training[target_t]\n",
    "    testing_X = data_testing[[x for x in data_testing if '_' in x and x != target_t]]\n",
    "    testing_y = data_testing[target_t]\n",
    "    scaler = StandardScaler()\n",
    "    training_X = scaler.fit_transform(training_X)\n",
    "    testing_X = scaler.transform(testing_X)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=[training_X.shape[1]]),\n",
    "            Dropout(0.3),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "    model.fit(training_X, training_y, epochs=200, validation_split=0.2, callbacks=[early_stopping])\n",
    "    testing_prediction = model.predict(testing_X)\n",
    "    r2 = round(r2_score(testing_y, testing_prediction), 2)\n",
    "    data_testing['c_prediction'] = testing_prediction\n",
    "    prediction = data_testing[[target, target_t, 'c_prediction']].copy()\n",
    "    prediction['prediction'] = prediction[target].shift(-t) + prediction['c_prediction']\n",
    "    prediction = prediction[[target, 'prediction']].dropna()\n",
    "    return prediction, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.0426\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 999us/step - loss: 0.0798 - val_loss: 0.0235\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 906us/step - loss: 0.0365 - val_loss: 0.0173\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 765us/step - loss: 0.0240 - val_loss: 0.0140\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 766us/step - loss: 0.0199 - val_loss: 0.0130\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 737us/step - loss: 0.0138 - val_loss: 0.0117\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0129 - val_loss: 0.0114\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 754us/step - loss: 0.0088 - val_loss: 0.0096\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 928us/step - loss: 0.0081 - val_loss: 0.0085\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 860us/step - loss: 0.0071 - val_loss: 0.0084\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0065 - val_loss: 0.0077\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0063 - val_loss: 0.0076\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0055 - val_loss: 0.0068\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0056 - val_loss: 0.0061\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0047 - val_loss: 0.0060\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0049 - val_loss: 0.0055\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0046 - val_loss: 0.0062\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0045 - val_loss: 0.0053\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 728us/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0040 - val_loss: 0.0051\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 715us/step - loss: 0.0041 - val_loss: 0.0057\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0039 - val_loss: 0.0052\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0039 - val_loss: 0.0057\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0037 - val_loss: 0.0051\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0036 - val_loss: 0.0050\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0032 - val_loss: 0.0044\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 732us/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0028 - val_loss: 0.0035\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0027 - val_loss: 0.0032\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0026 - val_loss: 0.0032\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0025 - val_loss: 0.0032\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0023 - val_loss: 0.0031\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 725us/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0023 - val_loss: 0.0034\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0021 - val_loss: 0.0031\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 848us/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0022 - val_loss: 0.0031\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0020 - val_loss: 0.0030\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0021 - val_loss: 0.0030\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 791us/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 86/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 87/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 88/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 89/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 90/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 91/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 92/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 93/200\n",
      "117/117 [==============================] - 0s 735us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 94/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 95/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 96/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 97/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0022 - val_loss: 0.0032\n",
      "Epoch 98/200\n",
      "117/117 [==============================] - 0s 715us/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 99/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 100/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 101/200\n",
      "117/117 [==============================] - 0s 912us/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 102/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 103/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 104/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 105/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 106/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 107/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0019 - val_loss: 0.0035\n",
      "Epoch 108/200\n",
      "117/117 [==============================] - 0s 728us/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 109/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 110/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 111/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0017 - val_loss: 0.0029\n",
      "15/15 [==============================] - 0s 394us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.2216 - val_loss: 0.0423\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0496 - val_loss: 0.0246\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0246 - val_loss: 0.0180\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0176 - val_loss: 0.0149\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0124 - val_loss: 0.0131\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 718us/step - loss: 0.0099 - val_loss: 0.0118\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 712us/step - loss: 0.0085 - val_loss: 0.0107\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0071 - val_loss: 0.0100\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0067 - val_loss: 0.0090\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0055 - val_loss: 0.0084\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0051 - val_loss: 0.0076\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0045 - val_loss: 0.0082\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0045 - val_loss: 0.0082\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0043 - val_loss: 0.0071\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0041 - val_loss: 0.0074\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0039 - val_loss: 0.0070\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0038 - val_loss: 0.0069\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0039 - val_loss: 0.0067\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0035 - val_loss: 0.0063\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0037 - val_loss: 0.0062\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0038 - val_loss: 0.0069\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 753us/step - loss: 0.0032 - val_loss: 0.0058\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0033 - val_loss: 0.0062\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0030 - val_loss: 0.0063\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0031 - val_loss: 0.0063\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0029 - val_loss: 0.0060\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0030 - val_loss: 0.0067\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0029 - val_loss: 0.0051\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0028 - val_loss: 0.0052\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 0.0027 - val_loss: 0.0053\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 725us/step - loss: 0.0029 - val_loss: 0.0057\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 677us/step - loss: 0.0027 - val_loss: 0.0051\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 939us/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 800us/step - loss: 0.0024 - val_loss: 0.0056\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0026 - val_loss: 0.0045\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0026 - val_loss: 0.0050\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 679us/step - loss: 0.0025 - val_loss: 0.0055\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0027 - val_loss: 0.0049\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0023 - val_loss: 0.0050\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0026 - val_loss: 0.0048\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0023 - val_loss: 0.0058\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0024 - val_loss: 0.0043\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0023 - val_loss: 0.0047\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0023 - val_loss: 0.0045\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0021 - val_loss: 0.0043\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0020 - val_loss: 0.0042\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0021 - val_loss: 0.0040\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0020 - val_loss: 0.0043\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0019 - val_loss: 0.0039\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0021 - val_loss: 0.0040\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0019 - val_loss: 0.0035\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0018 - val_loss: 0.0043\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0019 - val_loss: 0.0038\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 722us/step - loss: 0.0019 - val_loss: 0.0037\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0019 - val_loss: 0.0039\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0019 - val_loss: 0.0038\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0018 - val_loss: 0.0036\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0016 - val_loss: 0.0039\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 742us/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0016 - val_loss: 0.0033\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0014 - val_loss: 0.0031\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0014 - val_loss: 0.0041\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0027 - val_loss: 0.0032\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 746us/step - loss: 0.0015 - val_loss: 0.0037\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0015 - val_loss: 0.0030\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0014 - val_loss: 0.0030\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0014 - val_loss: 0.0042\n",
      "Epoch 86/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0014 - val_loss: 0.0030\n",
      "Epoch 87/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 88/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 89/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 90/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 91/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0014 - val_loss: 0.0030\n",
      "Epoch 92/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0014 - val_loss: 0.0033\n",
      "Epoch 93/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 94/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 95/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 96/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 97/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0013 - val_loss: 0.0032\n",
      "Epoch 98/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 99/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0014 - val_loss: 0.0033\n",
      "Epoch 100/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 101/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0012 - val_loss: 0.0036\n",
      "Epoch 102/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 103/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 104/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 105/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 106/200\n",
      "117/117 [==============================] - 0s 740us/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 107/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 108/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0032\n",
      "Epoch 109/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 110/200\n",
      "117/117 [==============================] - 0s 728us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 111/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 112/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 113/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "15/15 [==============================] - 0s 398us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.2957 - val_loss: 0.0497\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0673 - val_loss: 0.0237\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0383 - val_loss: 0.0141\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0237 - val_loss: 0.0108\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0176 - val_loss: 0.0097\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0155 - val_loss: 0.0089\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 679us/step - loss: 0.0122 - val_loss: 0.0080\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0112 - val_loss: 0.0076\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0098 - val_loss: 0.0068\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0095 - val_loss: 0.0066\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0085 - val_loss: 0.0060\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 679us/step - loss: 0.0081 - val_loss: 0.0059\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0077 - val_loss: 0.0054\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 732us/step - loss: 0.0072 - val_loss: 0.0052\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0064 - val_loss: 0.0049\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0063 - val_loss: 0.0045\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0064 - val_loss: 0.0045\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0059 - val_loss: 0.0046\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0053 - val_loss: 0.0045\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0059 - val_loss: 0.0046\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0055 - val_loss: 0.0036\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0053 - val_loss: 0.0036\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 951us/step - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 797us/step - loss: 0.0047 - val_loss: 0.0036\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0048 - val_loss: 0.0033\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0043 - val_loss: 0.0036\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0044 - val_loss: 0.0035\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 725us/step - loss: 0.0043 - val_loss: 0.0033\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0046 - val_loss: 0.0030\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0043 - val_loss: 0.0029\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0040 - val_loss: 0.0030\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0040 - val_loss: 0.0030\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0041 - val_loss: 0.0030\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0037 - val_loss: 0.0031\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 734us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0039 - val_loss: 0.0028\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0037 - val_loss: 0.0029\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0034 - val_loss: 0.0026\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0033 - val_loss: 0.0025\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 940us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 791us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0037 - val_loss: 0.0039\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 740us/step - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 746us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0032 - val_loss: 0.0027\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0031 - val_loss: 0.0027\n",
      "15/15 [==============================] - 0s 403us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.2503 - val_loss: 0.0395\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0487 - val_loss: 0.0230\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0241 - val_loss: 0.0186\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0149 - val_loss: 0.0167\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0123 - val_loss: 0.0159\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0098 - val_loss: 0.0148\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0085 - val_loss: 0.0144\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0072 - val_loss: 0.0140\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0067 - val_loss: 0.0135\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0061 - val_loss: 0.0133\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0057 - val_loss: 0.0126\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 936us/step - loss: 0.0055 - val_loss: 0.0126\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 803us/step - loss: 0.0050 - val_loss: 0.0123\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0049 - val_loss: 0.0122\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0048 - val_loss: 0.0116\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0045 - val_loss: 0.0115\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 786us/step - loss: 0.0041 - val_loss: 0.0112\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0039 - val_loss: 0.0116\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0040 - val_loss: 0.0113\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0037 - val_loss: 0.0109\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0036 - val_loss: 0.0106\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0034 - val_loss: 0.0106\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0034 - val_loss: 0.0111\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0034 - val_loss: 0.0107\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0033 - val_loss: 0.0106\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0032 - val_loss: 0.0102\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0033 - val_loss: 0.0103\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0031 - val_loss: 0.0098\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0029 - val_loss: 0.0098\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0029 - val_loss: 0.0099\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0030 - val_loss: 0.0096\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0027 - val_loss: 0.0090\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0028 - val_loss: 0.0093\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0027 - val_loss: 0.0092\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0025 - val_loss: 0.0089\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0025 - val_loss: 0.0087\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0023 - val_loss: 0.0093\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0024 - val_loss: 0.0087\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0084\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0025 - val_loss: 0.0081\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 727us/step - loss: 0.0024 - val_loss: 0.0084\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 764us/step - loss: 0.0022 - val_loss: 0.0080\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0023 - val_loss: 0.0080\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0021 - val_loss: 0.0086\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0023 - val_loss: 0.0076\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0020 - val_loss: 0.0078\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0024 - val_loss: 0.0080\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0021 - val_loss: 0.0075\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0021 - val_loss: 0.0075\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0021 - val_loss: 0.0072\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0021 - val_loss: 0.0073\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0022 - val_loss: 0.0067\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0019 - val_loss: 0.0069\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0019 - val_loss: 0.0068\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0020 - val_loss: 0.0071\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0019 - val_loss: 0.0069\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0020 - val_loss: 0.0066\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0020 - val_loss: 0.0067\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0020 - val_loss: 0.0069\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0017 - val_loss: 0.0068\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0018 - val_loss: 0.0062\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0063\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0019 - val_loss: 0.0064\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 745us/step - loss: 0.0018 - val_loss: 0.0064\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 733us/step - loss: 0.0017 - val_loss: 0.0063\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0018 - val_loss: 0.0064\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0019 - val_loss: 0.0069\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0017 - val_loss: 0.0064\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0017 - val_loss: 0.0066\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0017 - val_loss: 0.0061\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0017 - val_loss: 0.0059\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0017 - val_loss: 0.0061\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0016 - val_loss: 0.0063\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0017 - val_loss: 0.0061\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0016 - val_loss: 0.0056\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0016 - val_loss: 0.0062\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0017 - val_loss: 0.0056\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0016 - val_loss: 0.0064\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 949us/step - loss: 0.0015 - val_loss: 0.0060\n",
      "Epoch 86/200\n",
      "117/117 [==============================] - 0s 776us/step - loss: 0.0017 - val_loss: 0.0056\n",
      "Epoch 87/200\n",
      "117/117 [==============================] - 0s 725us/step - loss: 0.0015 - val_loss: 0.0059\n",
      "Epoch 88/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0015 - val_loss: 0.0058\n",
      "Epoch 89/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0015 - val_loss: 0.0060\n",
      "Epoch 90/200\n",
      "117/117 [==============================] - 0s 731us/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 91/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0014 - val_loss: 0.0058\n",
      "Epoch 92/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0015 - val_loss: 0.0058\n",
      "Epoch 93/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 94/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 95/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 96/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 97/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0014 - val_loss: 0.0056\n",
      "Epoch 98/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0015 - val_loss: 0.0057\n",
      "Epoch 99/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0014 - val_loss: 0.0055\n",
      "Epoch 100/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 101/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0014 - val_loss: 0.0051\n",
      "Epoch 102/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0014 - val_loss: 0.0051\n",
      "Epoch 103/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 104/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 105/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0015 - val_loss: 0.0057\n",
      "Epoch 106/200\n",
      "117/117 [==============================] - 0s 679us/step - loss: 0.0015 - val_loss: 0.0058\n",
      "Epoch 107/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 108/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 109/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0015 - val_loss: 0.0055\n",
      "Epoch 110/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 111/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0013 - val_loss: 0.0052\n",
      "Epoch 112/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0014 - val_loss: 0.0051\n",
      "Epoch 113/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0014 - val_loss: 0.0056\n",
      "Epoch 114/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 115/200\n",
      "117/117 [==============================] - 0s 733us/step - loss: 0.0013 - val_loss: 0.0053\n",
      "Epoch 116/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 117/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 118/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0013 - val_loss: 0.0053\n",
      "Epoch 119/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0013 - val_loss: 0.0053\n",
      "Epoch 120/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 121/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0013 - val_loss: 0.0054\n",
      "Epoch 122/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0013 - val_loss: 0.0056\n",
      "15/15 [==============================] - 0s 427us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.0556\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0851 - val_loss: 0.0292\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0435 - val_loss: 0.0208\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 729us/step - loss: 0.0275 - val_loss: 0.0180\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0218 - val_loss: 0.0161\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0149\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0145 - val_loss: 0.0141\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0140 - val_loss: 0.0131\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0131 - val_loss: 0.0121\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 717us/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 721us/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0084 - val_loss: 0.0094\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0082 - val_loss: 0.0096\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0080 - val_loss: 0.0096\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0075 - val_loss: 0.0083\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0069 - val_loss: 0.0076\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 754us/step - loss: 0.0067 - val_loss: 0.0075\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0058 - val_loss: 0.0069\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0059 - val_loss: 0.0069\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0058 - val_loss: 0.0072\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0056 - val_loss: 0.0068\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0057 - val_loss: 0.0069\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0054 - val_loss: 0.0068\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0054 - val_loss: 0.0064\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0051 - val_loss: 0.0062\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0049 - val_loss: 0.0062\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 752us/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 752us/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0048 - val_loss: 0.0057\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0047 - val_loss: 0.0055\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0042 - val_loss: 0.0052\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0042 - val_loss: 0.0055\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0041 - val_loss: 0.0061\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0041 - val_loss: 0.0050\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 751us/step - loss: 0.0039 - val_loss: 0.0054\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0039 - val_loss: 0.0049\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 717us/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0038 - val_loss: 0.0045\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.0049\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0037 - val_loss: 0.0050\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 727us/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0036 - val_loss: 0.0051\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0034 - val_loss: 0.0047\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0033 - val_loss: 0.0053\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0034 - val_loss: 0.0046\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 916us/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 86/200\n",
      "117/117 [==============================] - 0s 798us/step - loss: 0.0032 - val_loss: 0.0049\n",
      "Epoch 87/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0033 - val_loss: 0.0047\n",
      "Epoch 88/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0030 - val_loss: 0.0053\n",
      "Epoch 89/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0032 - val_loss: 0.0054\n",
      "Epoch 90/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 91/200\n",
      "117/117 [==============================] - 0s 755us/step - loss: 0.0031 - val_loss: 0.0051\n",
      "Epoch 92/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0032 - val_loss: 0.0050\n",
      "Epoch 93/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0031 - val_loss: 0.0050\n",
      "Epoch 94/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0032 - val_loss: 0.0048\n",
      "15/15 [==============================] - 0s 438us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4648 - val_loss: 0.0838\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 711us/step - loss: 0.1179 - val_loss: 0.0340\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 710us/step - loss: 0.0545 - val_loss: 0.0214\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0387 - val_loss: 0.0152\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0243 - val_loss: 0.0111\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0189 - val_loss: 0.0098\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0158 - val_loss: 0.0086\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0143 - val_loss: 0.0081\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.0069\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0104 - val_loss: 0.0063\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 760us/step - loss: 0.0097 - val_loss: 0.0060\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 714us/step - loss: 0.0089 - val_loss: 0.0058\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 737us/step - loss: 0.0081 - val_loss: 0.0053\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0075 - val_loss: 0.0051\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0073 - val_loss: 0.0047\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0065 - val_loss: 0.0042\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0066 - val_loss: 0.0041\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0059 - val_loss: 0.0038\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0058 - val_loss: 0.0037\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0054 - val_loss: 0.0035\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0054 - val_loss: 0.0034\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 705us/step - loss: 0.0052 - val_loss: 0.0032\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0047 - val_loss: 0.0031\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0048 - val_loss: 0.0031\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0045 - val_loss: 0.0030\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 725us/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0044 - val_loss: 0.0026\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0041 - val_loss: 0.0025\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0042 - val_loss: 0.0025\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0036 - val_loss: 0.0022\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0034 - val_loss: 0.0022\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 749us/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 748us/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0030 - val_loss: 0.0020\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0029 - val_loss: 0.0020\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0029 - val_loss: 0.0020\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0028 - val_loss: 0.0019\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0028 - val_loss: 0.0019\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0027 - val_loss: 0.0019\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0029 - val_loss: 0.0020\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 737us/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 745us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 732us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 742us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 739us/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 86/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 87/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 88/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 89/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 90/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 91/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 92/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 93/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 94/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 95/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 96/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 97/200\n",
      "117/117 [==============================] - 0s 736us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 98/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 99/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 100/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 101/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 102/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 103/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 104/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 105/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 106/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "15/15 [==============================] - 0s 418us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1765 - val_loss: 0.0334\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 712us/step - loss: 0.0418 - val_loss: 0.0201\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0223 - val_loss: 0.0159\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 755us/step - loss: 0.0175 - val_loss: 0.0142\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0145 - val_loss: 0.0128\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0125 - val_loss: 0.0120\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 706us/step - loss: 0.0095 - val_loss: 0.0102\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0091 - val_loss: 0.0098\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0089 - val_loss: 0.0092\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0082 - val_loss: 0.0088\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0077 - val_loss: 0.0086\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 709us/step - loss: 0.0073 - val_loss: 0.0085\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 0.0079\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 723us/step - loss: 0.0072 - val_loss: 0.0077\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0067 - val_loss: 0.0075\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0060 - val_loss: 0.0071\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0057 - val_loss: 0.0069\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0056 - val_loss: 0.0065\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 755us/step - loss: 0.0054 - val_loss: 0.0064\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0054 - val_loss: 0.0065\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0055 - val_loss: 0.0063\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 738us/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0050 - val_loss: 0.0058\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0046 - val_loss: 0.0057\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0045 - val_loss: 0.0056\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 0.0043 - val_loss: 0.0056\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0045 - val_loss: 0.0055\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0045 - val_loss: 0.0054\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0041 - val_loss: 0.0052\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0041 - val_loss: 0.0056\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 765us/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0041 - val_loss: 0.0054\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 706us/step - loss: 0.0040 - val_loss: 0.0052\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0039 - val_loss: 0.0052\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 744us/step - loss: 0.0039 - val_loss: 0.0052\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 705us/step - loss: 0.0038 - val_loss: 0.0053\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0039 - val_loss: 0.0053\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0038 - val_loss: 0.0051\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0036 - val_loss: 0.0052\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 0.0037 - val_loss: 0.0050\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 721us/step - loss: 0.0036 - val_loss: 0.0049\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0038 - val_loss: 0.0049\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 707us/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0038 - val_loss: 0.0049\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 705us/step - loss: 0.0035 - val_loss: 0.0050\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0035 - val_loss: 0.0051\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 712us/step - loss: 0.0035 - val_loss: 0.0049\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0035 - val_loss: 0.0049\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0036 - val_loss: 0.0051\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0035 - val_loss: 0.0049\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 760us/step - loss: 0.0036 - val_loss: 0.0050\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0033 - val_loss: 0.0049\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0035 - val_loss: 0.0049\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0033 - val_loss: 0.0049\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0033 - val_loss: 0.0052\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 749us/step - loss: 0.0034 - val_loss: 0.0052\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0050\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0033 - val_loss: 0.0051\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0033 - val_loss: 0.0051\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 0.0032 - val_loss: 0.0054\n",
      "15/15 [==============================] - 0s 397us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.3210 - val_loss: 0.0446\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 708us/step - loss: 0.0621 - val_loss: 0.0153\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 0.0294 - val_loss: 0.0078\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0163 - val_loss: 0.0051\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0100 - val_loss: 0.0039\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0074 - val_loss: 0.0032\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0056 - val_loss: 0.0029\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0038 - val_loss: 0.0027\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0036 - val_loss: 0.0025\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 0.0024\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 724us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 757us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 708us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 755us/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 9.7369e-04 - val_loss: 0.0015\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 9.9321e-04 - val_loss: 0.0013\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 9.0824e-04 - val_loss: 0.0013\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 8.9642e-04 - val_loss: 0.0012\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 712us/step - loss: 8.3508e-04 - val_loss: 0.0012\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 748us/step - loss: 8.8391e-04 - val_loss: 0.0013\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 8.0668e-04 - val_loss: 0.0012\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 8.1352e-04 - val_loss: 0.0012\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 8.3229e-04 - val_loss: 0.0011\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 706us/step - loss: 8.0513e-04 - val_loss: 0.0012\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 741us/step - loss: 7.9433e-04 - val_loss: 0.0011\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 8.5379e-04 - val_loss: 0.0012\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 7.7115e-04 - val_loss: 0.0011\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 7.5327e-04 - val_loss: 0.0011\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 7.3957e-04 - val_loss: 0.0011\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 7.7879e-04 - val_loss: 0.0011\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 735us/step - loss: 7.6002e-04 - val_loss: 9.6895e-04\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 7.4088e-04 - val_loss: 0.0011\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 7.3323e-04 - val_loss: 0.0011\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 6.8365e-04 - val_loss: 0.0011\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 6.7748e-04 - val_loss: 0.0011\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 7.0730e-04 - val_loss: 0.0011\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 6.9615e-04 - val_loss: 0.0010\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 6.7456e-04 - val_loss: 9.8959e-04\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 6.7313e-04 - val_loss: 9.7561e-04\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 710us/step - loss: 6.5002e-04 - val_loss: 0.0010\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 6.3423e-04 - val_loss: 0.0011\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 758us/step - loss: 6.6273e-04 - val_loss: 0.0010\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 6.3713e-04 - val_loss: 9.5615e-04\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 6.7204e-04 - val_loss: 9.8033e-04\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 707us/step - loss: 6.1192e-04 - val_loss: 9.7370e-04\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 6.0337e-04 - val_loss: 0.0010\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 5.9757e-04 - val_loss: 9.7496e-04\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 6.2744e-04 - val_loss: 9.1455e-04\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 767us/step - loss: 5.3904e-04 - val_loss: 9.6089e-04\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 6.0328e-04 - val_loss: 9.3640e-04\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 5.9056e-04 - val_loss: 0.0011\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 5.9295e-04 - val_loss: 9.0386e-04\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 761us/step - loss: 6.2847e-04 - val_loss: 8.9231e-04\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 5.6496e-04 - val_loss: 9.0777e-04\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 5.5444e-04 - val_loss: 8.9128e-04\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 5.5913e-04 - val_loss: 8.7208e-04\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 5.2400e-04 - val_loss: 8.8562e-04\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 5.1263e-04 - val_loss: 8.5985e-04\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 5.7335e-04 - val_loss: 0.0011\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 5.0223e-04 - val_loss: 8.7290e-04\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 5.5287e-04 - val_loss: 8.5999e-04\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 5.5258e-04 - val_loss: 9.7347e-04\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 729us/step - loss: 5.2518e-04 - val_loss: 8.5238e-04\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 5.4630e-04 - val_loss: 9.4457e-04\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 5.0736e-04 - val_loss: 8.4580e-04\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 5.2908e-04 - val_loss: 9.0757e-04\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 5.1523e-04 - val_loss: 9.3514e-04\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 5.3409e-04 - val_loss: 9.0043e-04\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 4.9571e-04 - val_loss: 9.3750e-04\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 4.7639e-04 - val_loss: 8.9948e-04\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 5.1072e-04 - val_loss: 8.8024e-04\n",
      "Epoch 86/200\n",
      "117/117 [==============================] - 0s 724us/step - loss: 5.1188e-04 - val_loss: 9.0880e-04\n",
      "Epoch 87/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 4.9588e-04 - val_loss: 0.0010\n",
      "Epoch 88/200\n",
      "117/117 [==============================] - 0s 776us/step - loss: 5.0088e-04 - val_loss: 8.7290e-04\n",
      "Epoch 89/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 5.0723e-04 - val_loss: 9.3986e-04\n",
      "Epoch 90/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 4.8546e-04 - val_loss: 8.6307e-04\n",
      "Epoch 91/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 4.8100e-04 - val_loss: 9.7390e-04\n",
      "Epoch 92/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 4.8815e-04 - val_loss: 8.6902e-04\n",
      "Epoch 93/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 4.5593e-04 - val_loss: 9.7999e-04\n",
      "Epoch 94/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 4.9487e-04 - val_loss: 8.5477e-04\n",
      "Epoch 95/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 4.8621e-04 - val_loss: 8.6946e-04\n",
      "Epoch 96/200\n",
      "117/117 [==============================] - 0s 704us/step - loss: 4.7631e-04 - val_loss: 8.4196e-04\n",
      "Epoch 97/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 4.4887e-04 - val_loss: 8.4818e-04\n",
      "Epoch 98/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 4.4996e-04 - val_loss: 9.5765e-04\n",
      "Epoch 99/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 4.5035e-04 - val_loss: 8.8684e-04\n",
      "Epoch 100/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 4.4743e-04 - val_loss: 8.6305e-04\n",
      "Epoch 101/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 4.3785e-04 - val_loss: 9.4241e-04\n",
      "Epoch 102/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 5.1801e-04 - val_loss: 8.9885e-04\n",
      "Epoch 103/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 4.5212e-04 - val_loss: 9.0394e-04\n",
      "Epoch 104/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 4.2918e-04 - val_loss: 9.8752e-04\n",
      "Epoch 105/200\n",
      "117/117 [==============================] - 0s 705us/step - loss: 4.5512e-04 - val_loss: 8.6981e-04\n",
      "Epoch 106/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 4.4408e-04 - val_loss: 8.9120e-04\n",
      "Epoch 107/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 4.5007e-04 - val_loss: 9.2781e-04\n",
      "Epoch 108/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 4.4475e-04 - val_loss: 9.3041e-04\n",
      "Epoch 109/200\n",
      "117/117 [==============================] - 0s 763us/step - loss: 4.4971e-04 - val_loss: 9.0245e-04\n",
      "Epoch 110/200\n",
      "117/117 [==============================] - 0s 744us/step - loss: 4.2810e-04 - val_loss: 8.9944e-04\n",
      "Epoch 111/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 4.1817e-04 - val_loss: 8.9038e-04\n",
      "Epoch 112/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 4.0775e-04 - val_loss: 8.6772e-04\n",
      "Epoch 113/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 4.3801e-04 - val_loss: 8.8861e-04\n",
      "Epoch 114/200\n",
      "117/117 [==============================] - 0s 700us/step - loss: 4.1150e-04 - val_loss: 9.2489e-04\n",
      "Epoch 115/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 4.3191e-04 - val_loss: 9.8744e-04\n",
      "Epoch 116/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 4.3009e-04 - val_loss: 9.0394e-04\n",
      "15/15 [==============================] - 0s 402us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.0630\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 708us/step - loss: 0.0896 - val_loss: 0.0285\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 0.0394 - val_loss: 0.0170\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0257 - val_loss: 0.0111\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0170 - val_loss: 0.0091\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 0.0079\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 953us/step - loss: 0.0108 - val_loss: 0.0069\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 825us/step - loss: 0.0097 - val_loss: 0.0066\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 733us/step - loss: 0.0083 - val_loss: 0.0062\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 741us/step - loss: 0.0071 - val_loss: 0.0054\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 779us/step - loss: 0.0065 - val_loss: 0.0051\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 742us/step - loss: 0.0062 - val_loss: 0.0048\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0057 - val_loss: 0.0047\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0056 - val_loss: 0.0045\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0051 - val_loss: 0.0044\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 730us/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0040 - val_loss: 0.0035\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 702us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 726us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 731us/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 750us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 671us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 667us/step - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 675us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 746us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 679us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 673us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 734us/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 678us/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 715us/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 721us/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 748us/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 729us/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 736us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 715us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 86/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 87/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "15/15 [==============================] - 0s 398us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1965 - val_loss: 0.0490\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0470 - val_loss: 0.0298\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0224 - val_loss: 0.0249\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0149 - val_loss: 0.0222\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0114 - val_loss: 0.0199\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0097 - val_loss: 0.0188\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0084 - val_loss: 0.0175\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0076 - val_loss: 0.0168\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 723us/step - loss: 0.0069 - val_loss: 0.0162\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0063 - val_loss: 0.0157\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 958us/step - loss: 0.0059 - val_loss: 0.0152\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 825us/step - loss: 0.0058 - val_loss: 0.0145\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0054 - val_loss: 0.0143\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 698us/step - loss: 0.0051 - val_loss: 0.0140\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0051 - val_loss: 0.0136\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 699us/step - loss: 0.0048 - val_loss: 0.0138\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 715us/step - loss: 0.0045 - val_loss: 0.0128\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0044 - val_loss: 0.0129\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 740us/step - loss: 0.0046 - val_loss: 0.0122\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0043 - val_loss: 0.0121\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0041 - val_loss: 0.0124\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 948us/step - loss: 0.0041 - val_loss: 0.0117\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 793us/step - loss: 0.0040 - val_loss: 0.0115\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0038 - val_loss: 0.0113\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0040 - val_loss: 0.0108\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0037 - val_loss: 0.0105\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0036 - val_loss: 0.0102\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0037 - val_loss: 0.0105\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 738us/step - loss: 0.0037 - val_loss: 0.0098\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0037 - val_loss: 0.0105\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0034 - val_loss: 0.0102\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0033 - val_loss: 0.0109\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0032 - val_loss: 0.0098\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0032 - val_loss: 0.0099\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0033 - val_loss: 0.0103\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0032 - val_loss: 0.0100\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0031 - val_loss: 0.0099\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 741us/step - loss: 0.0031 - val_loss: 0.0102\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0032 - val_loss: 0.0095\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0030 - val_loss: 0.0094\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 739us/step - loss: 0.0031 - val_loss: 0.0098\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0032 - val_loss: 0.0110\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0029 - val_loss: 0.0095\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0028 - val_loss: 0.0090\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 0.0090\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0028 - val_loss: 0.0095\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 750us/step - loss: 0.0029 - val_loss: 0.0098\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0029 - val_loss: 0.0100\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0028 - val_loss: 0.0093\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0028 - val_loss: 0.0094\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0027 - val_loss: 0.0095\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0026 - val_loss: 0.0089\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0027 - val_loss: 0.0100\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0028 - val_loss: 0.0098\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0026 - val_loss: 0.0092\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0026 - val_loss: 0.0091\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 684us/step - loss: 0.0025 - val_loss: 0.0087\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0025 - val_loss: 0.0087\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 677us/step - loss: 0.0025 - val_loss: 0.0091\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0026 - val_loss: 0.0085\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0024 - val_loss: 0.0093\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0025 - val_loss: 0.0095\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0024 - val_loss: 0.0090\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 774us/step - loss: 0.0025 - val_loss: 0.0090\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0092\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0024 - val_loss: 0.0092\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0024 - val_loss: 0.0094\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0024 - val_loss: 0.0090\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0023 - val_loss: 0.0089\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0024 - val_loss: 0.0091\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0024 - val_loss: 0.0093\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 751us/step - loss: 0.0022 - val_loss: 0.0088\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0023 - val_loss: 0.0091\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0023 - val_loss: 0.0086\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 913us/step - loss: 0.0022 - val_loss: 0.0085\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 805us/step - loss: 0.0022 - val_loss: 0.0094\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0023 - val_loss: 0.0090\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0022 - val_loss: 0.0087\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0022 - val_loss: 0.0090\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0023 - val_loss: 0.0104\n",
      "15/15 [==============================] - 0s 396us/step\n",
      "Epoch 1/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.1024\n",
      "Epoch 2/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0982 - val_loss: 0.0774\n",
      "Epoch 3/200\n",
      "117/117 [==============================] - 0s 701us/step - loss: 0.0543 - val_loss: 0.0629\n",
      "Epoch 4/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0384 - val_loss: 0.0605\n",
      "Epoch 5/200\n",
      "117/117 [==============================] - 0s 753us/step - loss: 0.0322 - val_loss: 0.0607\n",
      "Epoch 6/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0273 - val_loss: 0.0569\n",
      "Epoch 7/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0233 - val_loss: 0.0543\n",
      "Epoch 8/200\n",
      "117/117 [==============================] - 0s 740us/step - loss: 0.0227 - val_loss: 0.0540\n",
      "Epoch 9/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0207 - val_loss: 0.0536\n",
      "Epoch 10/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0198 - val_loss: 0.0510\n",
      "Epoch 11/200\n",
      "117/117 [==============================] - 0s 691us/step - loss: 0.0189 - val_loss: 0.0502\n",
      "Epoch 12/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0178 - val_loss: 0.0488\n",
      "Epoch 13/200\n",
      "117/117 [==============================] - 0s 697us/step - loss: 0.0163 - val_loss: 0.0464\n",
      "Epoch 14/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0158 - val_loss: 0.0452\n",
      "Epoch 15/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0146 - val_loss: 0.0431\n",
      "Epoch 16/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0155 - val_loss: 0.0424\n",
      "Epoch 17/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0400\n",
      "Epoch 18/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0134 - val_loss: 0.0414\n",
      "Epoch 19/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0136 - val_loss: 0.0401\n",
      "Epoch 20/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0130 - val_loss: 0.0393\n",
      "Epoch 21/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0125 - val_loss: 0.0381\n",
      "Epoch 22/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0122 - val_loss: 0.0373\n",
      "Epoch 23/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0121 - val_loss: 0.0382\n",
      "Epoch 24/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0119 - val_loss: 0.0370\n",
      "Epoch 25/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0120 - val_loss: 0.0377\n",
      "Epoch 26/200\n",
      "117/117 [==============================] - 0s 740us/step - loss: 0.0115 - val_loss: 0.0374\n",
      "Epoch 27/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0110 - val_loss: 0.0371\n",
      "Epoch 28/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0111 - val_loss: 0.0359\n",
      "Epoch 29/200\n",
      "117/117 [==============================] - 0s 771us/step - loss: 0.0107 - val_loss: 0.0353\n",
      "Epoch 30/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0109 - val_loss: 0.0357\n",
      "Epoch 31/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0106 - val_loss: 0.0344\n",
      "Epoch 32/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0099 - val_loss: 0.0349\n",
      "Epoch 33/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0098 - val_loss: 0.0338\n",
      "Epoch 34/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0100 - val_loss: 0.0348\n",
      "Epoch 35/200\n",
      "117/117 [==============================] - 0s 745us/step - loss: 0.0094 - val_loss: 0.0350\n",
      "Epoch 36/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0099 - val_loss: 0.0342\n",
      "Epoch 37/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0095 - val_loss: 0.0345\n",
      "Epoch 38/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0098 - val_loss: 0.0353\n",
      "Epoch 39/200\n",
      "117/117 [==============================] - 0s 695us/step - loss: 0.0091 - val_loss: 0.0354\n",
      "Epoch 40/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0095 - val_loss: 0.0336\n",
      "Epoch 41/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0094 - val_loss: 0.0338\n",
      "Epoch 42/200\n",
      "117/117 [==============================] - 0s 739us/step - loss: 0.0089 - val_loss: 0.0338\n",
      "Epoch 43/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0092 - val_loss: 0.0331\n",
      "Epoch 44/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0087 - val_loss: 0.0335\n",
      "Epoch 45/200\n",
      "117/117 [==============================] - 0s 679us/step - loss: 0.0087 - val_loss: 0.0322\n",
      "Epoch 46/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0086 - val_loss: 0.0325\n",
      "Epoch 47/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0086 - val_loss: 0.0324\n",
      "Epoch 48/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0085 - val_loss: 0.0321\n",
      "Epoch 49/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0086 - val_loss: 0.0312\n",
      "Epoch 50/200\n",
      "117/117 [==============================] - 0s 753us/step - loss: 0.0084 - val_loss: 0.0324\n",
      "Epoch 51/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0077 - val_loss: 0.0323\n",
      "Epoch 52/200\n",
      "117/117 [==============================] - 0s 730us/step - loss: 0.0083 - val_loss: 0.0328\n",
      "Epoch 53/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0079 - val_loss: 0.0331\n",
      "Epoch 54/200\n",
      "117/117 [==============================] - 0s 688us/step - loss: 0.0081 - val_loss: 0.0319\n",
      "Epoch 55/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0084 - val_loss: 0.0311\n",
      "Epoch 56/200\n",
      "117/117 [==============================] - 0s 686us/step - loss: 0.0077 - val_loss: 0.0321\n",
      "Epoch 57/200\n",
      "117/117 [==============================] - 0s 683us/step - loss: 0.0078 - val_loss: 0.0315\n",
      "Epoch 58/200\n",
      "117/117 [==============================] - 0s 744us/step - loss: 0.0076 - val_loss: 0.0320\n",
      "Epoch 59/200\n",
      "117/117 [==============================] - 0s 938us/step - loss: 0.0081 - val_loss: 0.0320\n",
      "Epoch 60/200\n",
      "117/117 [==============================] - 0s 760us/step - loss: 0.0081 - val_loss: 0.0309\n",
      "Epoch 61/200\n",
      "117/117 [==============================] - 0s 677us/step - loss: 0.0078 - val_loss: 0.0316\n",
      "Epoch 62/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0075 - val_loss: 0.0317\n",
      "Epoch 63/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0075 - val_loss: 0.0321\n",
      "Epoch 64/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0078 - val_loss: 0.0309\n",
      "Epoch 65/200\n",
      "117/117 [==============================] - 0s 681us/step - loss: 0.0076 - val_loss: 0.0309\n",
      "Epoch 66/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0071 - val_loss: 0.0314\n",
      "Epoch 67/200\n",
      "117/117 [==============================] - 0s 752us/step - loss: 0.0072 - val_loss: 0.0328\n",
      "Epoch 68/200\n",
      "117/117 [==============================] - 0s 690us/step - loss: 0.0068 - val_loss: 0.0323\n",
      "Epoch 69/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 0.0329\n",
      "Epoch 70/200\n",
      "117/117 [==============================] - 0s 679us/step - loss: 0.0071 - val_loss: 0.0326\n",
      "Epoch 71/200\n",
      "117/117 [==============================] - 0s 682us/step - loss: 0.0071 - val_loss: 0.0325\n",
      "Epoch 72/200\n",
      "117/117 [==============================] - 0s 703us/step - loss: 0.0071 - val_loss: 0.0310\n",
      "Epoch 73/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0067 - val_loss: 0.0319\n",
      "Epoch 74/200\n",
      "117/117 [==============================] - 0s 696us/step - loss: 0.0064 - val_loss: 0.0334\n",
      "Epoch 75/200\n",
      "117/117 [==============================] - 0s 737us/step - loss: 0.0069 - val_loss: 0.0318\n",
      "Epoch 76/200\n",
      "117/117 [==============================] - 0s 735us/step - loss: 0.0069 - val_loss: 0.0322\n",
      "Epoch 77/200\n",
      "117/117 [==============================] - 0s 680us/step - loss: 0.0070 - val_loss: 0.0328\n",
      "Epoch 78/200\n",
      "117/117 [==============================] - 0s 694us/step - loss: 0.0069 - val_loss: 0.0315\n",
      "Epoch 79/200\n",
      "117/117 [==============================] - 0s 687us/step - loss: 0.0068 - val_loss: 0.0333\n",
      "Epoch 80/200\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.0312\n",
      "Epoch 81/200\n",
      "117/117 [==============================] - 0s 693us/step - loss: 0.0063 - val_loss: 0.0329\n",
      "Epoch 82/200\n",
      "117/117 [==============================] - 0s 685us/step - loss: 0.0066 - val_loss: 0.0322\n",
      "Epoch 83/200\n",
      "117/117 [==============================] - 0s 745us/step - loss: 0.0069 - val_loss: 0.0321\n",
      "Epoch 84/200\n",
      "117/117 [==============================] - 0s 692us/step - loss: 0.0064 - val_loss: 0.0320\n",
      "Epoch 85/200\n",
      "117/117 [==============================] - 0s 689us/step - loss: 0.0061 - val_loss: 0.0326\n",
      "15/15 [==============================] - 0s 392us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2: Simple</th>\n",
       "      <th>r2: Multi NN</th>\n",
       "      <th>r2: Multi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Switzerland</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norway</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             r2: Simple  r2: Multi NN  r2: Multi\n",
       "Target                                          \n",
       "US                 0.86          0.91       0.94\n",
       "Germany            0.94          0.96       0.97\n",
       "UK                 0.72          0.84       0.89\n",
       "France             0.98          0.96       0.99\n",
       "Australia          0.78          0.87       0.90\n",
       "Canada             0.82          0.90       0.92\n",
       "New Zealand        0.73          0.82       0.87\n",
       "Japan              0.21          0.59       0.67\n",
       "Switzerland        0.62          0.81       0.83\n",
       "Norway             0.63          0.76       0.80\n",
       "Italy              0.80          0.86       0.89\n",
       "Mean               0.74          0.84       0.88"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def t_eval(t):\n",
    "    r2s = pd.DataFrame()\n",
    "    r2s['Target'] = list(codes.keys())\n",
    "    r2s['r2: Simple'] = r2s['Target'].apply(lambda x: simple(t)[f'y: {x}'][13])\n",
    "    r2s['r2: Multi NN'] = r2s['Target'].apply(lambda x: nn_multi(codes[x], t)[1])\n",
    "    r2s['r2: Multi'] = r2s['Target'].apply(lambda x: multi(codes[x], t)[1])\n",
    "    r2s = r2s.set_index('Target')\n",
    "    simple_mean = round(r2s['r2: Simple'].mean(), 2)\n",
    "    multi_mean = round(r2s['r2: Multi'].mean(), 2)\n",
    "    nn_mean = round(r2s['r2: Multi NN'].mean(), 2)\n",
    "    r2s.loc['Mean'] = {'r2: Simple': simple_mean, 'r2: Multi': multi_mean, 'r2: Multi NN': nn_mean}\n",
    "    return r2s\n",
    "\n",
    "t_eval(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>25</th>\n",
       "      <th>50</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Switzerland</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norway</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                1     5    10    25    50   100\n",
       "Target                                         \n",
       "US           0.84  0.92  0.94  0.93  0.93  0.85\n",
       "Germany      0.94  0.97  0.97  0.97  0.98  0.95\n",
       "UK           0.76  0.88  0.89  0.84  0.77  0.67\n",
       "France       0.98  0.99  0.99  0.99  0.98  0.98\n",
       "Australia    0.67  0.84  0.90  0.90  0.85  0.71\n",
       "Canada       0.83  0.90  0.92  0.90  0.93  0.92\n",
       "New Zealand  0.60  0.81  0.87  0.84  0.86  0.83\n",
       "Japan        0.37  0.62  0.67  0.65  0.68  0.48\n",
       "Switzerland  0.71  0.84  0.83  0.80  0.79  0.54\n",
       "Norway       0.64  0.76  0.80  0.86  0.89  0.85\n",
       "Italy        0.77  0.88  0.89  0.85  0.86  0.78\n",
       "Mean         0.74  0.86  0.88  0.87  0.87  0.78"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances = pd.DataFrame()\n",
    "performances['Target'] = list(codes.keys())\n",
    "\n",
    "for t in t_values:\n",
    "    performances[str(t)] = performances['Target'].apply(lambda x: multi(codes[x], t)[1])\n",
    "\n",
    "performances = performances.set_index('Target')\n",
    "performances.loc['Mean'] = round(performances.mean(),2)\n",
    "performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Develop trading strategy for given rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trader(name, t, threshold):\n",
    "    target = codes[name]\n",
    "    data, r2 = multi(target, t)\n",
    "    data['spread'] = data['prediction'] - data[target]\n",
    "    data['signal'] = 0\n",
    "    data.loc[data['prediction'] - threshold > data[target], 'signal'] = 1\n",
    "    data.loc[data['prediction'] < data[target] - threshold, 'signal'] = -1\n",
    "    data['signal'] = data['signal'].shift(1)\n",
    "    data['tar_return'] = data[target].diff(-1)\n",
    "    data['sig_return'] = data['tar_return'] * data['signal']\n",
    "    data['cum_return'] = data['sig_return'][::-1].cumsum()\n",
    "    data = data.dropna()\n",
    "    final_return = round(list(data['cum_return'])[0], 3)\n",
    "    flats = data['signal'].value_counts()[0]\n",
    "    return data, r2, flats, final_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>flats</th>\n",
       "      <th>final_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.85</td>\n",
       "      <td>13</td>\n",
       "      <td>1.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>0.95</td>\n",
       "      <td>35</td>\n",
       "      <td>-1.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>0.67</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>0.98</td>\n",
       "      <td>50</td>\n",
       "      <td>1.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>0.71</td>\n",
       "      <td>6</td>\n",
       "      <td>1.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>0.92</td>\n",
       "      <td>18</td>\n",
       "      <td>-2.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <td>0.83</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>0.48</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Switzerland</th>\n",
       "      <td>0.54</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norway</th>\n",
       "      <td>0.85</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>0.78</td>\n",
       "      <td>22</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               r2  flats  final_return\n",
       "Target                                \n",
       "US           0.85     13         1.793\n",
       "Germany      0.95     35        -1.427\n",
       "UK           0.67      8        -1.997\n",
       "France       0.98     50         1.348\n",
       "Australia    0.71      6         1.081\n",
       "Canada       0.92     18        -2.060\n",
       "New Zealand  0.83     16        -0.867\n",
       "Japan        0.48     25        -0.358\n",
       "Switzerland  0.54     21        -0.048\n",
       "Norway       0.85     12        -1.348\n",
       "Italy        0.78     22         0.902"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analysis(t, threshold):\n",
    "    analysis = pd.DataFrame()\n",
    "    analysis['Target'] = list(codes.keys())\n",
    "    analysis['r2'] = analysis['Target'].apply(lambda x: trader(x, t, threshold)[1])\n",
    "    analysis['flats'] = analysis['Target'].apply(lambda x: trader(x, t, threshold)[2])\n",
    "    analysis['final_return'] = analysis['Target'].apply(lambda x: trader(x, t, threshold)[3])\n",
    "    analysis = analysis.set_index('Target')\n",
    "    return analysis\n",
    "\n",
    "results = analysis(100, .01)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What makes the strategy work for some countries but not for others?\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "    plt.scatter(row['r2'], row['final_return'])\n",
    "    plt.text(row['r2'], row['final_return'], str(idx), ha='left', va='bottom')\n",
    "\n",
    "plt.title('Final Return vs r2')\n",
    "plt.xlabel('r2')\n",
    "plt.ylabel('Final Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "neural_network"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
